{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20a90598",
   "metadata": {},
   "source": [
    "# Feature Selection Cont'd\n",
    "In our proposal, we illustrate our motivation and part of feature selection.\n",
    "\n",
    "We can see that correlation among these factors are relatively high, which is easy to understand. In order to solve this problem, we adopt some particular feature selection functions to deal with this issue as can be seen in the following part.\n",
    "\n",
    "Here we build five models to [select features](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/tree/master/03%20feature%20selection):\n",
    "\n",
    "* [naiveSelection.py](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/03%20feature%20selection/naiveSelection.py)\n",
    "* [pcaSelection.py](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/03%20feature%20selection/pcaSelection.py)\n",
    "* [SVCL1Selection.py](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/03%20feature%20selection/SVCL1Selection.py)\n",
    "* [treeSelection.py](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/03%20feature%20selection/treeSelection.py)\n",
    "* [varianceThresholdSelection.py](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/03%20feature%20selection/varianceThresholdSelection.py)\n",
    "\n",
    "To avoid high correlation among features as much as possible, we can choose [LASSO in SVC model](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/03%20feature%20selection/SVCL1Selection.py). To find the most import features, we can choose PCA methods. Also, XGBoost includes feature selection itself. Moreover, to make it easy to call feature selection model, we encapsulate them as standard functions.\n",
    "\n",
    "Below is a sample feature selection function ([pcaSelection.py](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/03%20feature%20selection/pcaSelection.py)). As we can see, 12 PCA components can explain 82% of total variance, so we consider that 12 is the proper number of features to work with. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f490b79",
   "metadata": {},
   "source": [
    "Table 1. Portion explained by different numbers of components\n",
    "\n",
    "| Number of PCA components | Total explained variance |\n",
    "| ------------------------ | ------------------------ |\n",
    "| 6                        | 65%                      |\n",
    "| 8                        | 74%                      |\n",
    "| 10                       | 79%                      |\n",
    "| 12                       | 82%                      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d535bad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:30:48.544108Z",
     "start_time": "2021-07-17T04:30:44.962189Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pcaSelection(X_train, y_train, X_test, y_test, verbal = None, returnCoef = False):\n",
    "    '''\n",
    "    choose the feature selection method = 'pca'\n",
    "    fit any feature_selection model with the X_train, y_train\n",
    "    transform the X_train, X_test with the model\n",
    "    do not use the X_test to build feature selection model\n",
    "    \n",
    "    return the selected X_train, X_test\n",
    "    print info of the selecter\n",
    "    return the coef or the score of each feature if asked\n",
    "    \n",
    "    '''\n",
    "    #transform to standardscaler\n",
    "    features = X_train.columns.tolist()\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = pd.DataFrame(scaler.transform(X_train))\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test))\n",
    "    X_train.columns = features\n",
    "    X_test.columns = features\n",
    "    \n",
    "    pca = PCA(n_components = 12)\n",
    "    X_train = pca.fit_transform(X_train)\n",
    "    print ('The explained variance ratio is:')\n",
    "    print(pca.explained_variance_ratio_)\n",
    "    print('The total explained variance ratio is ')\n",
    "    print(sum(pca.explained_variance_ratio_))\n",
    "    print ('The explained variance is:')\n",
    "    print(pca.explained_variance_)\n",
    "    X_test = pca.transform(X_test)\n",
    "    \n",
    "    coef = pd.Series()\n",
    "    # featureName = None\n",
    "    \n",
    "    if verbal == True:\n",
    "       print('The total feature number is '+ str(X_train.shape[1]))\n",
    "       # print('The selected feature name is '+ str(featureName))\n",
    "       \n",
    "    if not returnCoef:\n",
    "        return(X_train, X_test)\n",
    "    else:\n",
    "        return(X_train, X_test, coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be9346",
   "metadata": {},
   "source": [
    "# PART3 Building Classifiers\n",
    "\n",
    "As we have already converted the problem into a classification prediction problem, we need to build [classifiers](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/tree/master/04%20build%20classifier%20model) based on machine learning algorithms to implement on the selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e05a2cd",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithms\n",
    "\n",
    "We implement [logistic regression](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/04%20build%20classifier%20model/MyClassifier.py), [naive Bayes](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/04%20build%20classifier%20model/MyClassifier.py), [KNN](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/04%20build%20classifier%20model/MyKNNClassifier.py), [perceptron](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/04%20build%20classifier%20model/MyClassifier.py), [decision tree](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/04%20build%20classifier%20model/MyDecisionTreeClassifier.py), [SVM](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/04%20build%20classifier%20model/MySVMClassifier.py), [XGBoost](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/04%20build%20classifier%20model/MyXGBoostClassifier.py) and [a Sequential neural network model in Keras](https://github.com/eiahb3838ya/PHBS_ML_for_quant_project/blob/master/04%20build%20classifier%20model/MyDeepLearningClassifier.py) to predict the rise or fall of Wind All A Index the next day. Below are some sample codes of implementing these algorithms as classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ee138a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:31:29.649854Z",
     "start_time": "2021-07-17T04:31:29.646854Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyLogisticRegClassifier:\n",
    "    def __init__(self):\n",
    "        self.parameter = self.getPara()\n",
    "        self.model = LogisticRegression()\n",
    "        \n",
    "    def getPara(self):\n",
    "        # do some how cv or things to decide the hyperparameter\n",
    "        return({})\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # do what ever plot or things you like \n",
    "        # just like your code\n",
    "        # self.model.fit(X,y)\n",
    "        return(self.model.fit(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1071282",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:31:31.740513Z",
     "start_time": "2021-07-17T04:31:31.374423Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-64de8a7ba28e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mparametersRepo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from parametersRepo import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "\n",
    "class MyXGBoostClassifier:\n",
    "    def __init__(self):\n",
    "        self.parameter = self.getPara()\n",
    "        self.model =  XGBClassifier(seed=self.parameter['model_seed'],\n",
    "                                    n_estimators=self.parameter['n_estimators'],\n",
    "                                    max_depth=self.parameter['max_depth'],\n",
    "                                    learning_rate=self.parameter['learning_rate'],\n",
    "                                    min_child_weight=self.parameter['min_child_weight'])\n",
    "        \n",
    "    def getPara(self):\n",
    "        # do some how cv or things to decide the hyperparameter\n",
    "        # n_neighbors = 15\n",
    "        # weights = 'uniform'\n",
    "        return(paraXGBoost)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # do what ever plot or things you like \n",
    "        # just like your code\n",
    "        self.model.fit(X,y)\n",
    "        print('The feature importance is :')\n",
    "        print(self.model.feature_importances_)\n",
    "        \n",
    "        plt.figure(figsize = (20, 6))\n",
    "        pyplot.bar(range(len(self.model.feature_importances_)),     self.model.feature_importances_)\n",
    "        pyplot.show()\n",
    "        plt.title('The feature importance')\n",
    "        plt.savefig('The feature importance')\n",
    "        plt.show()\n",
    "        # print('The total score of feature importance is:')\n",
    "        # print(sum(self.model.feature_importances_))\n",
    "        return(self.model.fit(X, y))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return(self.model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59905589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-17T04:31:41.666457Z",
     "start_time": "2021-07-17T04:31:41.648945Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'parametersRepo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4b2f5bcfc660>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mparametersRepo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'parametersRepo'"
     ]
    }
   ],
   "source": [
    "from parametersRepo import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras import models,layers\n",
    "import pandas as pd\n",
    "\n",
    "class MyDeepLearningClassifier:\n",
    "    def __init__(self):\n",
    "        self.parameter = None\n",
    "        self.model = None\n",
    "        \n",
    "    def getPara(self):\n",
    "        # do some how cv or things to decide the hyperparameter\n",
    "        # return dict\n",
    "        if self.parameter == None:\n",
    "            print('Hi~ please first use fit function to get model :)')\n",
    "        else:\n",
    "            print('haha! We already trained deepLearning Model~')\n",
    "            return self.parameter\n",
    "        return self.parameter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # do what ever plot or things you like \n",
    "        # just like your code\n",
    "        self.parameter = len(X.columns)\n",
    "        model = models.Sequential()\n",
    "        model.add(Dense(30,activation = 'relu',input_shape=(len(X.columns),)))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(1,activation = 'sigmoid' ))\n",
    "        # model.summary()\n",
    "        model.compile( loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
    "        self.model = model\n",
    "        return(self.model.fit(X, y,\n",
    "                              validation_split=0.2, \n",
    "                              epochs=1, batch_size=10, verbose=2))\n",
    "        \n",
    "    def predict(self, X):\n",
    "       \t return(pd.Series(self.model.predict_classes(X).flatten()).astype(bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f29a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b800b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
